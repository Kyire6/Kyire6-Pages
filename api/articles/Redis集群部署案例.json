{"title":"Redis 集群部署案例设计","slug":"Redis集群部署案例","date":"2022-06-06T15:33:22.000Z","updated":"2022-06-06T15:33:22.000Z","comments":true,"path":"api/articles/Redis集群部署案例.json","excerpt":null,"covers":["https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231141.png","https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231532.png","https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231612.png","https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231644.png","https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231817.png","https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231839.png","https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231915.png","https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606232459.png","https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606232446.png"],"content":"<h2 id=\"面试题：1-2-亿条数据需要缓存，请问如何设计这个存储案例？\"><a href=\"#面试题：1-2-亿条数据需要缓存，请问如何设计这个存储案例？\" class=\"headerlink\" title=\"面试题：1~2 亿条数据需要缓存，请问如何设计这个存储案例？\"></a>面试题：1~2 亿条数据需要缓存，请问如何设计这个存储案例？</h2><p>单机单台 100%不可能，肯定是分布式存储，用 redis 如何落地？</p>\n<h3 id=\"哈希取余分区\"><a href=\"#哈希取余分区\" class=\"headerlink\" title=\"哈希取余分区\"></a>哈希取余分区</h3><img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231141.png\" alt=\"image-20220606231134623\" style=\"zoom:50%;\" />\n\n<p>2 亿条记录就是 2 亿个 k,v，我们单机不行必须要分布式多机，假设有 3 台机器构成一个集群，用户每次读写操作都是根据公式：</p>\n<p>hash(key) % N 个机器台数，计算出哈希值，用来决定数据映射到哪一个节点上。</p>\n<ul>\n<li>优点</li>\n</ul>\n<p>简单粗暴，直接有效，只需要预估好数据规划好节点，例如 3 台、8 台、10 台，就能保证一段时间的数据支撑。使用 Hash 算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡+分而治之的作用。</p>\n<ul>\n<li>缺点</li>\n</ul>\n<p>原来规划好的节点，进行扩容或者缩容就比较麻烦了，不管扩缩，每次数据变动导致节点有变动，映射关系需要重新进行计算，在服务器个数固定不变时没有问题，如果需要弹性扩容或故障停机的情况下，原来的取模公式就会发生变化：Hash(key)/3 会变成 Hash(key) /?。此时地址经过取余运算的结果将发生很大变化，根据公式获取的服务器也会变得不可控。某个 redis 机器宕机了，由于台数数量变化，会导致 hash 取余全部数据重新洗牌。</p>\n<h3 id=\"一致性-Hash-算法分区\"><a href=\"#一致性-Hash-算法分区\" class=\"headerlink\" title=\"一致性 Hash 算法分区\"></a>一致性 Hash 算法分区</h3><blockquote>\n<p>一致性 Hash 算法背景</p>\n<p>一致性哈希算法在 1997 年由麻省理工学院中提出的，设计目标是为了解决分布式缓存数据变动和映射问题，某个机器宕机了，分母数量改变了，自然取余数不 OK 了。</p>\n</blockquote>\n<ol>\n<li>算法构建一致性哈希环</li>\n</ol>\n<p>一致性哈希环</p>\n<p>一致性哈希算法必然有个 hash 函数并按照算法产生 hash 值，这个算法的所有可能哈希值会构成一个全量集，这个集合可以成为一个 hash 空间[0,2^32-1]，这个是一个线性空间，但是在算法中，我们通过适当的逻辑控制将它首尾相连(0 = 2^32),这样让它逻辑上形成了一个环形空间。</p>\n<p>它也是按照使用取模的方法，前面笔记介绍的节点取模法是对节点（服务器）的数量进行取模。而一致性 Hash 算法是对 2^32 取模，简单来说，一致性 Hash 算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0-2^32-1（即哈希值是一个 32 位无符号整形），整个哈希环如下图：整个空间按顺时针方向组织，圆环的正上方的点代表 0，0 点右侧的第一个点代表 1，以此类推，2、3、4、……直到 2^32-1，也就是说 0 点左侧的第一个点代表 2^32-1， 0 和 2^32-1 在零点中方向重合，我们把这个由 2^32 个点组成的圆环称为 Hash 环。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231532.png\" alt=\"image-20220606231532166\" style=\"zoom:50%;\" />\n\n<ol start=\"2\">\n<li>服务器 IP 节点映射</li>\n</ol>\n<p>节点映射</p>\n<p>将集群中各个 IP 节点映射到环上的某一个位置。将各个服务器使用 Hash 进行一个哈希，具体可以选择服务器的 IP 或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假如 4 个节点 NodeA、B、C、D，经过 IP 地址的哈希函数计算(hash(ip))，使用 IP 地址哈希后在环空间的位置如下：</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231612.png\" alt=\"image-20220606231612002\" style=\"zoom:50%;\" />\n\n<ol start=\"3\">\n<li>key 落到服务器的落键规则</li>\n</ol>\n<p>当我们需要存储一个 kv 键值对时，首先计算 key 的 hash 值，hash(key)，将这个 key 使用相同的函数 Hash 计算出哈希值并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。</p>\n<p>如我们有 Object A、Object B、Object C、Object D 四个数据对象，经过哈希计算后，在环空间上的位置如下：根据一致性 Hash 算法，数据 A 会被定为到 Node A 上，B 被定为到 Node B 上，C 被定为到 Node C 上，D 被定为到 Node D 上。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231644.png\" alt=\"image-20220606231644182\" style=\"zoom:50%;\" />\n\n<ul>\n<li>优点</li>\n</ul>\n<p><strong>容错性</strong></p>\n<p>假设 Node C 宕机，可以看到此时对象 A、B、D 不会受到影响，只有 C 对象被重定位到 Node D。一般的，在一致性 Hash 算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。简单说，就是 C 挂了，受到影响的只是 B、C 之间的数据，并且这些数据会转移到 D 进行存储。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231817.png\" alt=\"image-20220606231816969\" style=\"zoom:50%;\" />\n\n<p><strong>扩展性</strong></p>\n<p>数据量增加了，需要增加一台节点 NodeX，X 的位置在 A 和 B 之间，那收到影响的也就是 A 到 X 之间的数据，重新把 A 到 X 的数据录入到 X 上即可，不会导致 hash 取余全部数据重新洗牌。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231839.png\" alt=\"image-20220606231839770\" style=\"zoom:50%;\" />\n\n<ul>\n<li>缺点</li>\n</ul>\n<p>Hash 环的数据倾斜问题</p>\n<p>一致性 Hash 算法在服务<strong>节点太少时</strong>，容易因为节点分布不均匀而造成<strong>数据倾斜</strong>（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器：</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231915.png\" alt=\"image-20220606231915547\" style=\"zoom:50%;\" />\n\n<blockquote>\n<p><strong>总结</strong></p>\n<p>为了在节点数目发生改变时尽可能少的迁移数据</p>\n<p>将所有的存储节点排列在收尾相接的 Hash 环上，每个 key 在计算 Hash 后会==顺时针==找到临近的存储节点存放。</p>\n<p>而当有节点加入或退出时仅影响该节点在 Hash 环上==顺时针相邻的后续节点==。</p>\n<ul>\n<li><strong>优点</strong></li>\n</ul>\n<p>加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。</p>\n<ul>\n<li><strong>缺点</strong></li>\n</ul>\n<p>数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。</p>\n</blockquote>\n<h3 id=\"哈希槽分区\"><a href=\"#哈希槽分区\" class=\"headerlink\" title=\"哈希槽分区\"></a>哈希槽分区</h3><blockquote>\n<p>==为什么出现？==</p>\n<p>解决一致性哈希算法出现的数据倾斜问题</p>\n<p>哈希槽实质就是一个数组，数组[0,2^14 -1]形成 hash slot 空间。</p>\n<p>==能干什么？==</p>\n<p>解决均匀分配的问题，在数据和节点之间又加入了一层，把这层称为哈希槽（slot），用于管理数据和节点之间的关系，现在就相当于节点上放的是槽，槽里放的是数据。</p>\n<p>槽解决的是粒度问题，相当于把粒度变大了，这样便于数据移动。</p>\n<p>哈希解决的是映射问题，使用 key 的哈希值来计算所在的槽，便于数据分配。</p>\n<p>==多少个哈希槽？==</p>\n<p>一个集群只能有 16384 个槽，编号 0-16383（0-2^14-1）。这些槽会分配给集群中的所有主节点，分配策略没有要求。可以指定哪些编号的槽分配给哪个主节点。集群会记录节点和槽的对应关系。解决了节点和槽的关系后，接下来就需要对 key 求哈希值，然后对 16384 取余，余数是几 key 就落入对应的槽里。slot = CRC16(key) % 16384。以槽为单位移动数据，因为槽的数目是固定的，处理起来比较容易，这样数据移动问题就解决了。</p>\n</blockquote>\n<p><strong>哈希槽计算</strong></p>\n<p>Redis 集群中内置了 16384 个哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，也就是映射到某个节点上。如下代码，key 之 A 、B 在 Node2， key 之 C 落在 Node3 上。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606232459.png\" alt=\"image-20220606232459710\" style=\"zoom:50%;\" />\n\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606232446.png\" alt=\"image-20220606232446308\" style=\"zoom:50%;\" />\n","more":"<h2 id=\"面试题：1-2-亿条数据需要缓存，请问如何设计这个存储案例？\"><a href=\"#面试题：1-2-亿条数据需要缓存，请问如何设计这个存储案例？\" class=\"headerlink\" title=\"面试题：1~2 亿条数据需要缓存，请问如何设计这个存储案例？\"></a>面试题：1~2 亿条数据需要缓存，请问如何设计这个存储案例？</h2><p>单机单台 100%不可能，肯定是分布式存储，用 redis 如何落地？</p>\n<h3 id=\"哈希取余分区\"><a href=\"#哈希取余分区\" class=\"headerlink\" title=\"哈希取余分区\"></a>哈希取余分区</h3><img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231141.png\" alt=\"image-20220606231134623\" style=\"zoom:50%;\" />\n\n<p>2 亿条记录就是 2 亿个 k,v，我们单机不行必须要分布式多机，假设有 3 台机器构成一个集群，用户每次读写操作都是根据公式：</p>\n<p>hash(key) % N 个机器台数，计算出哈希值，用来决定数据映射到哪一个节点上。</p>\n<ul>\n<li>优点</li>\n</ul>\n<p>简单粗暴，直接有效，只需要预估好数据规划好节点，例如 3 台、8 台、10 台，就能保证一段时间的数据支撑。使用 Hash 算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡+分而治之的作用。</p>\n<ul>\n<li>缺点</li>\n</ul>\n<p>原来规划好的节点，进行扩容或者缩容就比较麻烦了，不管扩缩，每次数据变动导致节点有变动，映射关系需要重新进行计算，在服务器个数固定不变时没有问题，如果需要弹性扩容或故障停机的情况下，原来的取模公式就会发生变化：Hash(key)/3 会变成 Hash(key) /?。此时地址经过取余运算的结果将发生很大变化，根据公式获取的服务器也会变得不可控。某个 redis 机器宕机了，由于台数数量变化，会导致 hash 取余全部数据重新洗牌。</p>\n<h3 id=\"一致性-Hash-算法分区\"><a href=\"#一致性-Hash-算法分区\" class=\"headerlink\" title=\"一致性 Hash 算法分区\"></a>一致性 Hash 算法分区</h3><blockquote>\n<p>一致性 Hash 算法背景</p>\n<p>一致性哈希算法在 1997 年由麻省理工学院中提出的，设计目标是为了解决分布式缓存数据变动和映射问题，某个机器宕机了，分母数量改变了，自然取余数不 OK 了。</p>\n</blockquote>\n<ol>\n<li>算法构建一致性哈希环</li>\n</ol>\n<p>一致性哈希环</p>\n<p>一致性哈希算法必然有个 hash 函数并按照算法产生 hash 值，这个算法的所有可能哈希值会构成一个全量集，这个集合可以成为一个 hash 空间[0,2^32-1]，这个是一个线性空间，但是在算法中，我们通过适当的逻辑控制将它首尾相连(0 = 2^32),这样让它逻辑上形成了一个环形空间。</p>\n<p>它也是按照使用取模的方法，前面笔记介绍的节点取模法是对节点（服务器）的数量进行取模。而一致性 Hash 算法是对 2^32 取模，简单来说，一致性 Hash 算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0-2^32-1（即哈希值是一个 32 位无符号整形），整个哈希环如下图：整个空间按顺时针方向组织，圆环的正上方的点代表 0，0 点右侧的第一个点代表 1，以此类推，2、3、4、……直到 2^32-1，也就是说 0 点左侧的第一个点代表 2^32-1， 0 和 2^32-1 在零点中方向重合，我们把这个由 2^32 个点组成的圆环称为 Hash 环。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231532.png\" alt=\"image-20220606231532166\" style=\"zoom:50%;\" />\n\n<ol start=\"2\">\n<li>服务器 IP 节点映射</li>\n</ol>\n<p>节点映射</p>\n<p>将集群中各个 IP 节点映射到环上的某一个位置。将各个服务器使用 Hash 进行一个哈希，具体可以选择服务器的 IP 或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假如 4 个节点 NodeA、B、C、D，经过 IP 地址的哈希函数计算(hash(ip))，使用 IP 地址哈希后在环空间的位置如下：</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231612.png\" alt=\"image-20220606231612002\" style=\"zoom:50%;\" />\n\n<ol start=\"3\">\n<li>key 落到服务器的落键规则</li>\n</ol>\n<p>当我们需要存储一个 kv 键值对时，首先计算 key 的 hash 值，hash(key)，将这个 key 使用相同的函数 Hash 计算出哈希值并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。</p>\n<p>如我们有 Object A、Object B、Object C、Object D 四个数据对象，经过哈希计算后，在环空间上的位置如下：根据一致性 Hash 算法，数据 A 会被定为到 Node A 上，B 被定为到 Node B 上，C 被定为到 Node C 上，D 被定为到 Node D 上。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231644.png\" alt=\"image-20220606231644182\" style=\"zoom:50%;\" />\n\n<ul>\n<li>优点</li>\n</ul>\n<p><strong>容错性</strong></p>\n<p>假设 Node C 宕机，可以看到此时对象 A、B、D 不会受到影响，只有 C 对象被重定位到 Node D。一般的，在一致性 Hash 算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。简单说，就是 C 挂了，受到影响的只是 B、C 之间的数据，并且这些数据会转移到 D 进行存储。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231817.png\" alt=\"image-20220606231816969\" style=\"zoom:50%;\" />\n\n<p><strong>扩展性</strong></p>\n<p>数据量增加了，需要增加一台节点 NodeX，X 的位置在 A 和 B 之间，那收到影响的也就是 A 到 X 之间的数据，重新把 A 到 X 的数据录入到 X 上即可，不会导致 hash 取余全部数据重新洗牌。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231839.png\" alt=\"image-20220606231839770\" style=\"zoom:50%;\" />\n\n<ul>\n<li>缺点</li>\n</ul>\n<p>Hash 环的数据倾斜问题</p>\n<p>一致性 Hash 算法在服务<strong>节点太少时</strong>，容易因为节点分布不均匀而造成<strong>数据倾斜</strong>（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器：</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606231915.png\" alt=\"image-20220606231915547\" style=\"zoom:50%;\" />\n\n<blockquote>\n<p><strong>总结</strong></p>\n<p>为了在节点数目发生改变时尽可能少的迁移数据</p>\n<p>将所有的存储节点排列在收尾相接的 Hash 环上，每个 key 在计算 Hash 后会==顺时针==找到临近的存储节点存放。</p>\n<p>而当有节点加入或退出时仅影响该节点在 Hash 环上==顺时针相邻的后续节点==。</p>\n<ul>\n<li><strong>优点</strong></li>\n</ul>\n<p>加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。</p>\n<ul>\n<li><strong>缺点</strong></li>\n</ul>\n<p>数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。</p>\n</blockquote>\n<h3 id=\"哈希槽分区\"><a href=\"#哈希槽分区\" class=\"headerlink\" title=\"哈希槽分区\"></a>哈希槽分区</h3><blockquote>\n<p>==为什么出现？==</p>\n<p>解决一致性哈希算法出现的数据倾斜问题</p>\n<p>哈希槽实质就是一个数组，数组[0,2^14 -1]形成 hash slot 空间。</p>\n<p>==能干什么？==</p>\n<p>解决均匀分配的问题，在数据和节点之间又加入了一层，把这层称为哈希槽（slot），用于管理数据和节点之间的关系，现在就相当于节点上放的是槽，槽里放的是数据。</p>\n<p>槽解决的是粒度问题，相当于把粒度变大了，这样便于数据移动。</p>\n<p>哈希解决的是映射问题，使用 key 的哈希值来计算所在的槽，便于数据分配。</p>\n<p>==多少个哈希槽？==</p>\n<p>一个集群只能有 16384 个槽，编号 0-16383（0-2^14-1）。这些槽会分配给集群中的所有主节点，分配策略没有要求。可以指定哪些编号的槽分配给哪个主节点。集群会记录节点和槽的对应关系。解决了节点和槽的关系后，接下来就需要对 key 求哈希值，然后对 16384 取余，余数是几 key 就落入对应的槽里。slot = CRC16(key) % 16384。以槽为单位移动数据，因为槽的数目是固定的，处理起来比较容易，这样数据移动问题就解决了。</p>\n</blockquote>\n<p><strong>哈希槽计算</strong></p>\n<p>Redis 集群中内置了 16384 个哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，也就是映射到某个节点上。如下代码，key 之 A 、B 在 Node2， key 之 C 落在 Node3 上。</p>\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606232459.png\" alt=\"image-20220606232459710\" style=\"zoom:50%;\" />\n\n<img src=\"https://my-typora-oss.oss-cn-shanghai.aliyuncs.com/image-master/20220606232446.png\" alt=\"image-20220606232446308\" style=\"zoom:50%;\" />\n","categories":[{"name":"中间件","path":"api/categories/中间件.json"}],"tags":[{"name":"笔记","path":"api/tags/笔记.json"},{"name":"面试","path":"api/tags/面试.json"},{"name":"Redis","path":"api/tags/Redis.json"}]}